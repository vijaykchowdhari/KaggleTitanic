{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Kaggle Titanic Competition\n",
    "In this project, I will be looking at Kaggle Titanic Competition and submitting my piece of code. Kaggle is a site where people create algorithms and compete against machine learning practitioners around the world. Your algorithm wins if it's the most accurate on a particular data set. \n",
    "\n",
    "To begin the problem we have to think logically about the columns and what we're trying to predict. What variables might logically affect the outcome of survival? Reading more about the Titanic disaster might help you with this.\n",
    "\n",
    "We know that women and children were more likely to survive, so Age and Sex are probably good predictors. It's also logical to think that passenger class might affect the outcome, because first class cabins were closer to the deck of the ship. While fare is tied to passenger class and will probably have a strong correlation with it, it may also give us some useful information.\n",
    "\n",
    "Family size (the number of siblings and parents/children) will probably be correlated with survival one way or the other. That's because there would either be more people to help you, or more people to think about trying to save.\n",
    "\n",
    "There may be links between survival and columns like Ticket, Name, and Embarked (because people who boarded at certain ports may have had cabins closer or farther away from the top of the ship), .\n",
    "\n",
    "We call this step acquiring domain knowledge, and it's fairly important to most machine learning tasks. We're looking to engineer the features so that we maximize the information we have about what we're trying to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examining the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "titanic = pd.read_csv(\"C:/Users/Jennifer/Documents/Python/Data/titanic_train.csv\")\n",
    "\n",
    "# Print the first five rows of the dataframe\n",
    "print(titanic.head(5))\n",
    "print(titanic.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning missing data\n",
    "There are many ways to clean up missing data. One of the easiest is to fill in all of the missing values with the median of all the values in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling non-numeric columns\n",
    "Several of our columns are non-numeric, which is a problem when it comes time to make predictions. We can't feed non-numeric columns into a machine learning algorithm and expect it to make sense of them.\n",
    "\n",
    "We either have to exclude the non-numeric columns when we train our algorithm (Name, Sex, Cabin, Embarked, and Ticket), or find a way to convert them to numeric columns.\n",
    "\n",
    "We'll ignore the Ticket, Cabin, and Name columns because we can't extract much information from them. Most of the values in the cabin column are missing (there are only 204 values out of a total of 891 rows), and it probably isn't a particularly informative column anyway. The Ticket and Name columns are unlikely to tell us much without some domain knowledge about what the ticket numbers mean, and about which names correlate with characteristics like large or rich families."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the Sex Column to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['male' 'female']\n"
     ]
    }
   ],
   "source": [
    "# Find all of the unique genders \n",
    "# The column appears to contain the values male and female only\n",
    "print(titanic[\"Sex\"].unique())\n",
    "\n",
    "# Replace all the occurences of male with the number 0\n",
    "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeating the same for the embarked column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'C' 'Q' nan]\n"
     ]
    }
   ],
   "source": [
    "# Find all of the unique values for \"Embarked\"\n",
    "print(titanic[\"Embarked\"].unique())\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "titanic.loc[titanic[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions using Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jennifer\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import the linear regression class\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Sklearn also has a helper that makes it easy to do cross-validation\n",
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "# The columns we'll use to predict the target\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm class\n",
    "alg = LinearRegression()\n",
    "# Generate cross-validation folds for the titanic data set\n",
    "# It returns the row indices corresponding to train and test\n",
    "# We set random_state to ensure we get the same splits every time we run this\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    # The predictors we're using to train the algorithm  \n",
    "    # Note how we only take the rows in the train folds\n",
    "    train_predictors = (titanic[predictors].iloc[train,:])\n",
    "    # The target we're using to train the algorithm\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    # Training the algorithm using the predictors and target\n",
    "    alg.fit(train_predictors, train_target)\n",
    "    # We can now make predictions on the test fold\n",
    "    test_predictions = alg.predict(titanic[predictors].iloc[test,:])\n",
    "    predictions.append(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Prediction Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-b980ddc0ef2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# The predictions are in three separate NumPy arrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Concatenate them into a single array, along the axis 0 (the only 1 axis)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Map predictions to outcomes (the only possible outcomes are 1 and 0)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The predictions are in three separate NumPy arrays  \n",
    "# Concatenate them into a single array, along the axis 0 (the only 1 axis) \n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Map predictions to outcomes (the only possible outcomes are 1 and 0)\n",
    "predictions[predictions > .5] = 1\n",
    "predictions[predictions <=.5] = 0\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "We have our first predictions! With only 78.3% accuracy, though, they aren't very good. Instead, we can use logistic regression to output values between 0 and 1.\n",
    "\n",
    "Logistic regression takes the output of a linear regression and maps it to a probability value between 0 and 1. It does the mapping using the logit function. Passing any value through the logit function will map it to a value between 0 and 1 by \"squeezing\" the extreme values. This is perfect for us, because we only care about two outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787878787879\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import cross_validation\n",
    "\n",
    "# Initialize our algorithm\n",
    "alg = LogisticRegression(random_state=1)\n",
    "# Compute the accuracy score for all the cross-validation folds; this is much simpler than what we did before\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the test data set\n",
    "Our accuracy is decent, but not great. We can still try a few things to make it better, and we'll talk about them in the next mission.\n",
    "\n",
    "For now, we'll focus on performing the exact same steps on the test data that we did on the training data. If we don't perform the exact same operations, then we won't be able to make valid predictions on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic_test = pd.read_csv(\"C:/Users/Jennifer/Documents/Python/Data/titanic_test.csv\")\n",
    "titanic_test[\"Age\"] = titanic_test[\"Age\"].fillna(titanic[\"Age\"].median())\n",
    "titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(titanic_test[\"Fare\"].median())\n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"male\", \"Sex\"] = 0 \n",
    "titanic_test.loc[titanic_test[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "titanic_test[\"Embarked\"] = titanic_test[\"Embarked\"].fillna(\"S\")\n",
    "\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic_test.loc[titanic_test[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the predictions on the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the algorithm class\n",
    "alg = LogisticRegression(random_state=1)\n",
    "\n",
    "# Train the algorithm using all the training data\n",
    "alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Make predictions using the test set\n",
    "predictions = alg.predict(titanic_test[predictors])\n",
    "\n",
    "# Create a new dataframe with only the columns Kaggle wants from the data set\n",
    "submission = pd.DataFrame({\n",
    "        \"PassengerId\": titanic_test[\"PassengerId\"],\n",
    "        \"Survived\": predictions\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived\n",
      "0            892         0\n",
      "1            893         0\n",
      "2            894         0\n",
      "3            895         0\n",
      "4            896         1\n",
      "5            897         0\n",
      "6            898         1\n",
      "7            899         0\n",
      "8            900         1\n",
      "9            901         0\n",
      "10           902         0\n",
      "11           903         0\n",
      "12           904         1\n",
      "13           905         0\n",
      "14           906         1\n",
      "15           907         1\n",
      "16           908         0\n",
      "17           909         0\n",
      "18           910         1\n",
      "19           911         1\n",
      "20           912         0\n",
      "21           913         0\n",
      "22           914         1\n",
      "23           915         1\n",
      "24           916         1\n",
      "25           917         0\n",
      "26           918         1\n",
      "27           919         0\n",
      "28           920         0\n",
      "29           921         0\n",
      "..           ...       ...\n",
      "388         1280         0\n",
      "389         1281         0\n",
      "390         1282         1\n",
      "391         1283         1\n",
      "392         1284         0\n",
      "393         1285         0\n",
      "394         1286         0\n",
      "395         1287         1\n",
      "396         1288         0\n",
      "397         1289         1\n",
      "398         1290         0\n",
      "399         1291         0\n",
      "400         1292         1\n",
      "401         1293         0\n",
      "402         1294         1\n",
      "403         1295         1\n",
      "404         1296         0\n",
      "405         1297         0\n",
      "406         1298         0\n",
      "407         1299         0\n",
      "408         1300         1\n",
      "409         1301         1\n",
      "410         1302         1\n",
      "411         1303         1\n",
      "412         1304         1\n",
      "413         1305         0\n",
      "414         1306         1\n",
      "415         1307         0\n",
      "416         1308         0\n",
      "417         1309         0\n",
      "\n",
      "[418 rows x 2 columns]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Random Forest Model\n",
    "In the last code cell, we made our first submission to Titanic: Machine Learning from Disaster, a machine learning competition on Kaggle.\n",
    "\n",
    "Our submission wasn't very high-scoring, though. There are three main ways we can improve it:\n",
    "\n",
    "Use a better machine learning algorithm.\n",
    "Generate better features.\n",
    "Combine multiple machine learning algorithms.\n",
    "\n",
    "In this mission, I will do all three. First, we'll find a different algorithm to use, instead of logistic regression. This time, we'll use the random forests algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.785634118967\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "# Initialize our algorithm with the default paramters\n",
    "# n_estimators is the number of trees we want to make\n",
    "# min_samples_split is the minimum number of rows we need to make a split\n",
    "# min_samples_leaf is the minimum number of samples we can have at the place where a tree branch ends (the bottom points of the tree)\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "# Compute the accuracy score for all of the cross validation folds; this is much simpler than what we did before\n",
    "kf = cross_validation.KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=kf)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Parameters to improve accuracy\n",
    "The first (and easiest) thing we can do to improve the accuracy of the random forest is to increase the number of trees we're using. Training more trees will take more time, but because we're averaging many predictions we made on different subsets of the data, having more trees will greatly increase accuracy (up to a point).\n",
    "\n",
    "We can also tweak the min_samples_split and min_samples_leaf variables to reduce overfitting. Because of the way a decision tree works, very deep splits in a tree can make it fit to quirks in the data set, rather than true signal.\n",
    "\n",
    "For this reason, increasing min_samples_split and min_samples_leaf can reduce overfitting. This will actually improve our score because we're making predictions on unseen data. A model that's less overfit and can generalize better will actually perform better on unseen data, but worse on seen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.81593714927\n"
     ]
    }
   ],
   "source": [
    "alg = RandomForestClassifier(random_state=1, n_estimators=50, min_samples_split=4, min_samples_leaf=2)\n",
    "# Compute the accuracy score for all the cross-validation folds; this is much simpler than what we did before\n",
    "kf = cross_validation.KFold(titanic.shape[0], 3, random_state=1)\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=kf)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating New Features with Apply\n",
    "We can also generate new features. Here are some ideas:\n",
    "\n",
    "- The length of the name. This could pertain to how rich the person was, and therefore their position on the Titanic.\n",
    "- The total number of people in a family (SibSp + Parch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating a familysize column\n",
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "\n",
    "# The .apply method generates a new series\n",
    "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the Passenger Titles with a Regular Expression\n",
    "We can extract the passengers' titles from their names. The titles take the form of Master., Mr., Mrs., etc. There are a few very common titles, and a \"long tail\" of titles that only one or two passengers have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Mlle          2\n",
      "Col           2\n",
      "Major         2\n",
      "Ms            1\n",
      "Countess      1\n",
      "Capt          1\n",
      "Sir           1\n",
      "Lady          1\n",
      "Jonkheer      1\n",
      "Mme           1\n",
      "Don           1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# A function to get the title from a name\n",
    "def get_title(name):\n",
    "    # Use a regular expression to search for a title  \n",
    "    # Titles always consist of capital and lowercase letters, and end with a period\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "# Get all of the titles, and print how often each one occurs\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pd.value_counts(titles))\n",
    "\n",
    "# Map each title to an integer  \n",
    "# Some titles are very rare, so they're compressed into the same codes as other titles\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "\n",
    "# Verify that we converted everything\n",
    "print(pd.value_counts(titles))\n",
    "\n",
    "# Add in the title column\n",
    "titanic[\"Title\"] = titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating a feature for Family Groups\n",
    "We can also generate a feature that indicates which family passengers belong to. Because survival was probably very dependent on your family and the people around you, this has a good chance of being a helpful feature.\n",
    "\n",
    "To create this feature, we'll concatenate each passenger's last name with FamilySize to get a unique family ID. Then we'll be able to assign a code to each person based on their family ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1      800\n",
      " 14       8\n",
      " 149      7\n",
      " 63       6\n",
      " 50       6\n",
      " 59       6\n",
      " 17       5\n",
      " 384      4\n",
      " 27       4\n",
      " 25       4\n",
      " 162      4\n",
      " 8        4\n",
      " 84       4\n",
      " 340      4\n",
      " 43       3\n",
      " 269      3\n",
      " 58       3\n",
      " 633      2\n",
      " 167      2\n",
      " 280      2\n",
      " 510      2\n",
      " 90       2\n",
      " 83       1\n",
      " 625      1\n",
      " 376      1\n",
      " 449      1\n",
      " 498      1\n",
      " 588      1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "\n",
    "# A dictionary mapping family name to ID\n",
    "family_id_mapping = {}\n",
    "\n",
    "# A function to get the ID for a particular row\n",
    "def get_family_id(row):\n",
    "    # Find the last name by splitting on a comma\n",
    "    last_name = row[\"Name\"].split(\",\")[0]\n",
    "    # Create the family ID\n",
    "    family_id = \"{0}{1}\".format(last_name, row[\"FamilySize\"])\n",
    "    # Look up the ID in the mapping\n",
    "    if family_id not in family_id_mapping:\n",
    "        if len(family_id_mapping) == 0:\n",
    "            current_id = 1\n",
    "        else:\n",
    "            # Get the maximum ID from the mapping, and add 1 to it if we don't have an ID\n",
    "            current_id = (max(family_id_mapping.items(), key=operator.itemgetter(1))[1] + 1)\n",
    "        family_id_mapping[family_id] = current_id\n",
    "    return family_id_mapping[family_id]\n",
    "\n",
    "# Get the family IDs with the apply method\n",
    "family_ids = titanic.apply(get_family_id, axis=1)\n",
    "\n",
    "# There are a lot of family IDs, so we'll compress all of the families with less than three members into one code\n",
    "family_ids[titanic[\"FamilySize\"] < 3] = -1\n",
    "\n",
    "# Print the count of each unique ID\n",
    "print(pd.value_counts(family_ids))\n",
    "\n",
    "titanic[\"FamilyId\"] = family_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifing the best features to use\n",
    "Feature engineering is the most important part of any machine learning task, and there are a lot more features we could calculate. However, we also need a way to figure out which features are the best.\n",
    "\n",
    "One way to accomplish this is to use univariate feature selection. This approach essentially involves reviewing a data set column by column to identify the ones that correlate most closely with what we're trying to predict (Survived).\n",
    "\n",
    "As usual, sklearn has a function that will help us with feature selection. The SelectKBest function selects the best features from the data. We can specify how many features we want this function to select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEyCAYAAADqYisiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/H/oQT4f1kgsJAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x6a59bd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.817059483726\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\", \"NameLength\"]\n",
    "\n",
    "# Perform feature selection\n",
    "selector = SelectKBest(f_classif, k=5)\n",
    "selector.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "\n",
    "# Get the raw p-values for each feature, and transform them from p-values into scores\n",
    "scores = -np.log10(selector.pvalues_)\n",
    "\n",
    "# Plot the scores  \n",
    "# Do you see how \"Pclass\", \"Sex\", \"Title\", and \"Fare\" are the best features?\n",
    "plt.bar(range(len(predictors)), scores)\n",
    "plt.xticks(range(len(predictors)), predictors, rotation='vertical')\n",
    "plt.show()\n",
    "\n",
    "# Pick only the four best features\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=50, min_samples_split=8, min_samples_leaf=4)\n",
    "# Compute the accuracy score for all the cross-validation folds; this is much simpler than what we did before\n",
    "scores = cross_validation.cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=3)\n",
    "\n",
    "# Take the mean of the scores (because we have one for each fold)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions with multiple classifiers\n",
    "One thing we can do to improve the accuracy of our predictions is ensemble different classifiers. Ensembling means generating predictions based on information from a set of classifiers, instead of just one. In practice, this means that we average their predictions.\n",
    "\n",
    "Generally speaking, the more diverse the models we ensemble, the higher our accuracy will be. Diversity means that the models generate their results from different columns, or use very different methods to generate predictions. Ensembling a random forest classifier with a decision tree probably won't work extremely well, because they're very similar. On the other hand, ensembling a linear regression with a random forest can yield very good results.\n",
    "\n",
    "One caveat with ensembling is that the classifiers we use have to be about the same in terms of accuracy. Ensembling one classifier that's much less accurate than the other will probably make the final result worse.\n",
    "\n",
    "In this case, we'll ensemble logistic regression we trained on the most linear predictors (the ones that have a linear order, as well as some correlation to Survived) with a gradient-boosted tree we trained on all of the predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.278338945006\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import numpy as np\n",
    "\n",
    "# The algorithms we want to ensemble\n",
    "# We're using the more linear predictors for the logistic regression, and everything with the gradient boosting classifier\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "# Initialize the cross-validation folds\n",
    "kf = KFold(titanic.shape[0], n_folds=3, random_state=1)\n",
    "\n",
    "predictions = []\n",
    "for train, test in kf:\n",
    "    train_target = titanic[\"Survived\"].iloc[train]\n",
    "    full_test_predictions = []\n",
    "    # Make predictions for each algorithm on each fold\n",
    "    for alg, predictors in algorithms:\n",
    "        # Fit the algorithm on the training data\n",
    "        alg.fit(titanic[predictors].iloc[train,:], train_target)\n",
    "        # Select and predict on the test fold \n",
    "        # We need to use .astype(float) to convert the dataframe to all floats and avoid an sklearn error\n",
    "        test_predictions = alg.predict_proba(titanic[predictors].iloc[test,:].astype(float))[:,1]\n",
    "        full_test_predictions.append(test_predictions)\n",
    "    # Use a simple ensembling scheme&#8212;just average the predictions to get the final classification\n",
    "    test_predictions = (full_test_predictions[0] + full_test_predictions[1]) / 2\n",
    "    # Any value over .5 is assumed to be a 1 prediction, and below .5 is a 0 prediction\n",
    "    test_predictions[test_predictions <= .5] = 0\n",
    "    test_predictions[test_predictions > .5] = 1\n",
    "    predictions.append(test_predictions)\n",
    "\n",
    "# Put all the predictions together into one array\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "\n",
    "# Compute accuracy by comparing to the training data\n",
    "accuracy = sum(predictions[predictions == titanic[\"Survived\"]]) / len(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making changes to the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     240\n",
      "2      79\n",
      "3      72\n",
      "4      21\n",
      "7       2\n",
      "6       2\n",
      "10      1\n",
      "5       1\n",
      "Name: Title, dtype: int64\n",
      "{'Braund1': 1, 'Cumings1': 2, 'Heikkinen0': 3, 'Futrelle1': 4, 'Allen0': 5, 'Moran0': 6, 'McCarthy0': 7, 'Palsson4': 8, 'Johnson2': 9, 'Nasser1': 10, 'Sandstrom2': 11, 'Bonnell0': 12, 'Saundercock0': 13, 'Andersson6': 14, 'Vestrom0': 15, 'Hewlett0': 16, 'Rice5': 17, 'Williams0': 18, 'Vander Planke1': 19, 'Masselmani0': 20, 'Fynney0': 21, 'Beesley0': 22, 'McGowan0': 23, 'Sloper0': 24, 'Asplund6': 25, 'Emir0': 26, 'Fortune5': 27, \"O'Dwyer0\": 28, 'Todoroff0': 29, 'Uruchurtu0': 30, 'Spencer1': 31, 'Glynn0': 32, 'Wheadon0': 33, 'Meyer1': 34, 'Holverson1': 35, 'Mamee0': 36, 'Cann0': 37, 'Vander Planke2': 38, 'Nicola-Yarred1': 39, 'Ahlin1': 40, 'Turpin1': 41, 'Kraeff0': 42, 'Laroche3': 43, 'Devaney0': 44, 'Rogers0': 45, 'Lennon1': 46, \"O'Driscoll0\": 47, 'Samaan2': 48, 'Arnold-Franchi1': 49, 'Panula5': 50, 'Nosworthy0': 51, 'Harper1': 52, 'Faunthorpe1': 53, 'Ostby1': 54, 'Woolner0': 55, 'Rugg0': 56, 'Novel0': 57, 'West3': 58, 'Goodwin7': 59, 'Sirayanian0': 60, 'Icard0': 61, 'Harris1': 62, 'Skoog5': 63, 'Stewart0': 64, 'Moubarek2': 65, 'Nye0': 66, 'Crease0': 67, 'Kink2': 68, 'Jenkin0': 69, 'Hood0': 70, 'Chronopoulos1': 71, 'Bing0': 72, 'Moen0': 73, 'Staneff0': 74, 'Moutal0': 75, 'Caldwell2': 76, 'Dowdell0': 77, 'Waelens0': 78, 'Sheerlinck0': 79, 'McDermott0': 80, 'Carrau0': 81, 'Ilett0': 82, 'Backstrom3': 83, 'Ford4': 84, 'Slocovski0': 85, 'Celotti0': 86, 'Christmann0': 87, 'Andreasson0': 88, 'Chaffee1': 89, 'Dean3': 90, 'Coxon0': 91, 'Shorney0': 92, 'Goldschmidt0': 93, 'Greenfield1': 94, 'Doling1': 95, 'Kantor1': 96, 'Petranec0': 97, 'Petroff0': 98, 'White1': 99, 'Johansson0': 100, 'Gustafsson2': 101, 'Mionoff0': 102, 'Salkjelsvik0': 103, 'Moss0': 104, 'Rekic0': 105, 'Moran1': 106, 'Porter0': 107, 'Zabour1': 108, 'Barton0': 109, 'Jussila1': 110, 'Attalah0': 111, 'Pekoniemi0': 112, 'Connors0': 113, 'Baxter1': 114, 'Hickman2': 115, 'Moore0': 116, 'Webber0': 117, 'McMahon0': 118, 'Madsen0': 119, 'Peter2': 120, 'Ekstrom0': 121, 'Drazenoic0': 122, 'Coelho0': 123, 'Robins1': 124, 'Weisz1': 125, 'Sobey0': 126, 'Richard0': 127, 'Newsom2': 128, 'Osen0': 129, 'Giglio0': 130, 'Boulos2': 131, 'Nysten0': 132, 'Hakkarainen1': 133, 'Burke0': 134, 'Andrew0': 135, 'Nicholls2': 136, 'Andersson0': 137, 'Navratil2': 138, 'Byles0': 139, 'Bateman0': 140, 'Pears1': 141, 'Meo0': 142, 'van Billiard2': 143, 'Olsen0': 144, 'Williams1': 145, 'Gilnagh0': 146, 'Corn0': 147, 'Smiljanic0': 148, 'Sage10': 149, 'Cribb1': 150, 'Watt0': 151, 'Bengtsson0': 152, 'Calic0': 153, 'Goldsmith2': 154, 'Chibnall1': 155, 'Baumann0': 156, 'Ling0': 157, 'Van der hoef0': 158, 'Sivola0': 159, 'Smith0': 160, 'Klasen2': 161, 'Lefebre4': 162, 'Isham0': 163, 'Hale0': 164, 'Leonard0': 165, 'Pernot0': 166, 'Becker3': 167, 'Kink-Heilmann2': 168, 'Rood0': 169, \"O'Brien1\": 170, 'Romaine0': 171, 'Bourke2': 172, 'Turcin0': 173, 'Pinsky0': 174, 'Carbines0': 175, 'Andersen-Jensen1': 176, 'Brown0': 177, 'Lurette0': 178, 'Mernagh0': 179, 'Olsen1': 180, 'Madigan0': 181, 'Yrois0': 182, 'Vande Walle0': 183, 'Johanson0': 184, 'Youseff0': 185, 'Cohen0': 186, 'Strom1': 187, 'Backstrom1': 188, 'Albimona0': 189, 'Carr0': 190, 'Blank0': 191, 'Ali0': 192, 'Cameron0': 193, 'Perkin0': 194, 'Givard0': 195, 'Kiernan1': 196, 'Newell1': 197, 'Honkanen0': 198, 'Jacobsohn1': 199, 'Bazzani0': 200, 'Harris0': 201, 'Sunderland0': 202, 'Bracken0': 203, 'Green0': 204, 'Nenkoff0': 205, 'Hoyt1': 206, 'Berglund0': 207, 'Mellors0': 208, 'Lovell0': 209, 'Fahlstrom0': 210, 'Larsson0': 211, 'Sjostedt0': 212, 'Leyson0': 213, 'Harknett0': 214, 'Hold1': 215, 'Collyer2': 216, 'Pengelly0': 217, 'Hunt0': 218, 'Murphy1': 219, 'Coleridge0': 220, 'Maenpaa0': 221, 'Minahan2': 222, 'Lindahl0': 223, 'Hamalainen2': 224, 'Beckwith2': 225, 'Carter1': 226, 'Reed0': 227, 'Strom2': 228, 'Stead0': 229, 'Lobb1': 230, 'Rosblom2': 231, 'Touma2': 232, 'Thorne0': 233, 'Cherry0': 234, 'Ward0': 235, 'Parrish1': 236, 'Taussig2': 237, 'Harrison0': 238, 'Henry0': 239, 'Reeves0': 240, 'Persson1': 241, 'Graham1': 242, 'Bissette0': 243, 'Cairns0': 244, 'Tornquist0': 245, 'Mellinger1': 246, 'Natsch1': 247, 'Healy0': 248, 'Andrews1': 249, 'Lindblom0': 250, 'Parkes0': 251, 'Abbott2': 252, 'Duane0': 253, 'Olsson0': 254, 'de Pelsmaeker0': 255, 'Dorking0': 256, 'Stankovic0': 257, 'de Mulder0': 258, 'Naidenoff0': 259, 'Hosono0': 260, 'Connolly0': 261, 'Barber0': 262, 'Bishop1': 263, 'Levy0': 264, 'Haas0': 265, 'Mineff0': 266, 'Lewy0': 267, 'Hanna0': 268, 'Allison3': 269, 'Saalfeld0': 270, 'Kelly0': 271, 'McCoy2': 272, 'Johnson0': 273, 'Keane0': 274, 'Fleming0': 275, 'Penasco y Castellana1': 276, 'Abelson1': 277, 'Francatelli0': 278, 'Hays0': 279, 'Ryerson4': 280, 'Lahtinen2': 281, 'Hendekovic0': 282, 'Hart2': 283, 'Nilsson0': 284, 'Moraweck0': 285, 'Wick2': 286, 'Spedden2': 287, 'Dennis0': 288, 'Danoff0': 289, 'Slayter0': 290, 'Young0': 291, 'Nysveen0': 292, 'Ball0': 293, 'Hippach1': 294, 'Partner0': 295, 'Frauenthal1': 296, 'Denkoff0': 297, 'Burns0': 298, 'Dahl0': 299, 'Blackwell0': 300, 'Collander0': 301, 'Sedgwick0': 302, 'Fox0': 303, 'Davison1': 304, 'Coutts2': 305, 'Dimic0': 306, 'Odahl0': 307, 'Williams-Lambert0': 308, 'Elias2': 309, 'Yousif0': 310, 'Vanden Steen0': 311, 'Bowerman1': 312, 'Funk0': 313, 'McGovern0': 314, 'Mockler0': 315, 'del Carlo1': 316, 'Barbara1': 317, 'Asim0': 318, 'Adahl0': 319, 'Warren1': 320, 'Moussa0': 321, 'Jermyn0': 322, 'Aubart0': 323, 'Harder1': 324, 'Wiklund1': 325, 'Beavan0': 326, 'Ringhini0': 327, 'Landergren0': 328, 'Widener2': 329, 'Betros0': 330, 'Gustafsson0': 331, 'Bidois0': 332, 'Nakid2': 333, 'Tikkanen0': 334, 'Plotcharsky0': 335, 'Davies0': 336, 'Buss0': 337, 'Sadlier0': 338, 'Lehmann0': 339, 'Carter3': 340, 'Jansson0': 341, 'McKane0': 342, 'Pain0': 343, 'Trout0': 344, 'Niskanen0': 345, 'Adams0': 346, 'Oreskovic0': 347, 'Gale1': 348, 'Widegren0': 349, 'Richards2': 350, 'Birkeland0': 351, 'Sdycoff0': 352, 'Hart0': 353, 'Minahan1': 354, 'Cunningham0': 355, 'Sundman0': 356, 'Meek0': 357, 'Drew2': 358, 'Silven2': 359, 'Matthews0': 360, 'Van Impe2': 361, 'Gheorgheff0': 362, 'Charters0': 363, 'Zimmerman0': 364, 'Danbom2': 365, 'Wiseman0': 366, 'Clarke1': 367, 'Phillips0': 368, 'Flynn0': 369, 'Pickard0': 370, 'Bjornstrom-Steffansson0': 371, 'Thorneycroft1': 372, 'Louch1': 373, 'Kallio0': 374, 'Silvey1': 375, 'Richards5': 376, 'Kvillner0': 377, 'Hampe0': 378, 'Petterson1': 379, 'Reynaldo0': 380, 'Johannesen-Bratthammer0': 381, 'Dodge2': 382, 'Seward0': 383, 'Baclini3': 384, 'Peuchen0': 385, 'Hagland1': 386, 'Foreman0': 387, 'Goldenberg1': 388, 'Peduzzi0': 389, 'Jalsevac0': 390, 'Millet0': 391, 'Kenyon1': 392, 'Toomey0': 393, \"O'Connor0\": 394, 'Anderson0': 395, 'Morley0': 396, 'Gee0': 397, 'Milling0': 398, 'Maisner0': 399, 'Goncalves0': 400, 'Campbell0': 401, 'Smart0': 402, 'Scanlan0': 403, 'Keefe0': 404, 'Cacic0': 405, 'Jerwan0': 406, 'Strandberg0': 407, 'Clifford0': 408, 'Renouf1': 409, 'Karlsson0': 410, 'Hirvonen1': 411, 'Frost0': 412, 'Rouse0': 413, 'Turkula0': 414, 'Kent0': 415, 'Somerton0': 416, 'Windelov0': 417, 'Molson0': 418, 'Artagaveytia0': 419, 'Stanley0': 420, 'Yousseff0': 421, 'Eustis1': 422, 'Shellard0': 423, 'Svensson0': 424, 'Canavan0': 425, \"O'Sullivan0\": 426, 'Laitinen0': 427, 'Maioni0': 428, 'Quick2': 429, 'Bradley0': 430, 'Lang0': 431, 'Daly0': 432, 'McGough0': 433, 'Rothschild1': 434, 'Coleff0': 435, 'Walker0': 436, 'Lemore0': 437, 'Ryan0': 438, 'Angle1': 439, 'Pavlovic0': 440, 'Perreault0': 441, 'Vovk0': 442, 'Lahoud0': 443, 'Kassem0': 444, 'Farrell0': 445, 'Ridsdale0': 446, 'Farthing0': 447, 'Salonen0': 448, 'Hocking3': 449, 'Toufik0': 450, 'Butt0': 451, 'LeRoy0': 452, 'Risien0': 453, 'Frolicher2': 454, 'Crosby2': 455, 'Beane1': 456, 'Douglas1': 457, 'Nicholson0': 458, 'Padro y Manent0': 459, 'Davies2': 460, 'Thayer2': 461, 'Sharp0': 462, \"O'Brien0\": 463, 'Leeni0': 464, 'Ohman0': 465, 'Wright0': 466, 'Duff Gordon1': 467, 'Robbins0': 468, 'de Messemaeker1': 469, 'Morrow0': 470, 'Sivic0': 471, 'Norman0': 472, 'Simmons0': 473, 'Meanwell0': 474, 'Stoytcheff0': 475, 'Doharr0': 476, 'Jonsson0': 477, 'Appleton2': 478, 'Rush0': 479, 'Patchett0': 480, 'Garside0': 481, 'Caram1': 482, 'Jussila0': 483, 'Christy2': 484, 'Downton0': 485, 'Ross0': 486, 'Paulner0': 487, 'Jarvis0': 488, 'Frolicher-Stehli2': 489, 'Gilinski0': 490, 'Murdlin0': 491, 'Rintamaki0': 492, 'Stephenson1': 493, 'Elsbury0': 494, 'Chapman1': 495, 'Leitch0': 496, 'Boulos0': 497, 'Jacobsohn3': 498, 'Slabenoff0': 499, 'Harrington0': 500, 'Torber0': 501, 'Homer0': 502, 'Lindell1': 503, 'Karaic0': 504, 'Daniel0': 505, 'Shutes0': 506, 'Jardin0': 507, 'Horgan0': 508, 'Brocklebank0': 509, 'Herman3': 510, 'Gavey0': 511, 'Yasbeck1': 512, 'Kimball1': 513, 'Hansen0': 514, 'Bowen0': 515, 'Sutton0': 516, 'Kirkland0': 517, 'Longley0': 518, 'Bostandyeff0': 519, \"O'Connell0\": 520, 'Barkworth0': 521, 'Lundahl0': 522, 'Stahelin-Maeglin0': 523, 'Parr0': 524, 'Davis0': 525, 'Leinonen0': 526, 'Jensen0': 527, 'Sagesser0': 528, 'Foo0': 529, 'Cor0': 530, 'Simonius-Blumer0': 531, 'Willey0': 532, 'Mitkoff0': 533, 'Kalvik0': 534, \"O'Leary0\": 535, 'Hegarty0': 536, 'Radeff0': 537, 'Eitemiller0': 538, 'Newell2': 539, 'Frauenthal2': 540, 'Badt0': 541, 'Colley0': 542, 'Lindqvist1': 543, 'Butler0': 544, 'Rommetvedt0': 545, 'Cook0': 546, 'Taylor1': 547, 'Brown2': 548, 'Davidson1': 549, 'Mitchell0': 550, 'Wilhelms0': 551, 'Watson0': 552, 'Edvardsson0': 553, 'Sawyer0': 554, 'Turja0': 555, 'Cardeza1': 556, 'Peters0': 557, 'Hassab0': 558, 'Olsvigen0': 559, 'Dakic0': 560, 'Fischer0': 561, 'Madill1': 562, 'Dick1': 563, 'Karun1': 564, 'Lam0': 565, 'Saad0': 566, 'Weir0': 567, 'Chapman0': 568, 'Mullens0': 569, 'Humblen0': 570, 'Astor1': 571, 'Silverthorne0': 572, 'Gallagher0': 573, 'Hansen1': 574, 'Calderhead0': 575, 'Cleaver0': 576, 'Mayne0': 577, 'Klaber0': 578, 'Greenberg0': 579, 'Soholt0': 580, 'Endres0': 581, 'Troutt0': 582, 'McEvoy0': 583, 'Jensen1': 584, 'Gillespie0': 585, 'Hodges0': 586, 'Chambers1': 587, 'Renouf3': 588, 'Mannion0': 589, 'Bryhl1': 590, 'Ilmakangas1': 591, 'Hassan0': 592, 'Knight0': 593, 'Berriman0': 594, 'Troupiansky0': 595, 'Lesurer0': 596, 'Ivanoff0': 597, 'Nankoff0': 598, 'Hawksford0': 599, 'Cavendish1': 600, 'McNamee1': 601, 'Stranden0': 602, 'Sinkkonen0': 603, 'Marvin1': 604, 'Connaghton0': 605, 'Wells2': 606, 'Moor1': 607, 'Vande Velde0': 608, 'Jonkoff0': 609, 'Carlsson0': 610, 'Bailey0': 611, 'Theobald0': 612, 'Rothes0': 613, 'Garfirth0': 614, 'Nirva0': 615, 'Barah0': 616, 'Eklund0': 617, 'Hogeboom1': 618, 'Brewe0': 619, 'Mangan0': 620, 'Gronnestad0': 621, 'Lievens0': 622, 'Mack0': 623, 'Elias0': 624, 'Hocking4': 625, 'Myhrman0': 626, 'Tobin0': 627, 'Emanuel0': 628, 'Kilgannon0': 629, 'Robert1': 630, 'Ayoub0': 631, 'Long0': 632, 'Johnston3': 633, 'Harmer0': 634, 'Sjoblom0': 635, 'Guggenheim0': 636, 'Gaskell0': 637, 'Hoyt0': 638, 'Dantcheff0': 639, 'Otter0': 640, 'Leader0': 641, 'Osman0': 642, 'Ibrahim Shawah0': 643, 'Ponesell0': 644, 'Thomas1': 645, 'Hedman0': 646, 'Andrews0': 647, 'Pettersson0': 648, 'Meyer0': 649, 'Alexander0': 650, 'Lester0': 651, 'Slemen0': 652, 'Tomlin0': 653, 'Fry0': 654, 'Heininen0': 655, 'Mallet2': 656, 'Holm0': 657, 'Hays2': 658, 'Lulic0': 659, 'Reuchlin0': 660, 'McCormack0': 661, 'Stone0': 662, 'Augustsson0': 663, 'Allum0': 664, 'Compton2': 665, 'Pasic0': 666, 'Sirota0': 667, 'Chip0': 668, 'Marechal0': 669, 'Alhomaki0': 670, 'Mudd0': 671, 'Serepeca0': 672, 'Lemberopolous0': 673, 'Culumovic0': 674, 'Abbing0': 675, 'Markoff0': 676, 'Lines1': 677, 'Aks1': 678, 'Razi0': 679, 'Hansen2': 680, 'Giles1': 681, 'Swift0': 682, 'Gill0': 683, 'Bystrom0': 684, 'Duran y More1': 685, 'Roebling0': 686, 'van Melkebeke0': 687, 'Balkic0': 688, 'Vander Cruyssen0': 689, 'Najib0': 690, 'Laleff0': 691, 'Potter1': 692, 'Shelley1': 693, 'Markun0': 694, 'Dahlberg0': 695, 'Banfield0': 696, 'Sutehall0': 697, 'Montvila0': 698, 'Graham0': 699, 'Behr0': 700, 'Dooley0': 701}\n"
     ]
    }
   ],
   "source": [
    "# First, we'll add titles to the test set\n",
    "titles = titanic_test[\"Name\"].apply(get_title)\n",
    "# We're adding the Dona title to the mapping, because it's in the test set, but not the training set\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2, \"Dona\": 10}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "titanic_test[\"Title\"] = titles\n",
    "# Check the counts of each unique title\n",
    "print(pd.value_counts(titanic_test[\"Title\"]))\n",
    "\n",
    "# Now we add the family size column\n",
    "titanic_test[\"FamilySize\"] = titanic_test[\"SibSp\"] + titanic_test[\"Parch\"]\n",
    "\n",
    "# Now we can add family IDs\n",
    "# We'll use the same IDs we used earlier\n",
    "print(family_id_mapping)\n",
    "\n",
    "family_ids = titanic_test.apply(get_family_id, axis=1)\n",
    "family_ids[titanic_test[\"FamilySize\"] < 3] = -1\n",
    "titanic_test[\"FamilyId\"] = family_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"Fare\", \"Embarked\", \"FamilySize\", \"Title\", \"FamilyId\"]\n",
    "\n",
    "algorithms = [\n",
    "    [GradientBoostingClassifier(random_state=1, n_estimators=25, max_depth=3), predictors],\n",
    "    [LogisticRegression(random_state=1), [\"Pclass\", \"Sex\", \"Fare\", \"FamilySize\", \"Title\", \"Age\", \"Embarked\"]]\n",
    "]\n",
    "\n",
    "full_predictions = []\n",
    "for alg, predictors in algorithms:\n",
    "    # Fit the algorithm using the full training data.\n",
    "    alg.fit(titanic[predictors], titanic[\"Survived\"])\n",
    "    # Predict using the test dataset.  We have to convert all the columns to floats to avoid an error\n",
    "    predictions = alg.predict_proba(titanic_test[predictors].astype(float))[:,1]\n",
    "    full_predictions.append(predictions)\n",
    "\n",
    "# The gradient boosting classifier generates better predictions, so we weight it higher\n",
    "predictions = (full_predictions[0] * 3 + full_predictions[1]) / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "There's still more work I could have done in feature engineering:\n",
    "\n",
    "- Try using features related to the cabins.\n",
    "- See if any family size features might help. Do the number of women in a family make the entire family more likely to survive?\n",
    "- Does the national origin of the passenger's name have anything to do with survival?\n",
    "\n",
    "There's also a lot more we can do on the algorithm side:\n",
    "\n",
    "- Try the random forest classifier in the ensemble.\n",
    "- A support vector machine might work well with this data.\n",
    "- We could try neural networks.\n",
    "- Boosting with a different base classifier might work better.\n",
    "\n",
    "And with ensembling methods:\n",
    "\n",
    "- Could majority voting be a better ensembling method than averaging probabilities?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
